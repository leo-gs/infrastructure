{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import getpass\n",
    "import json\n",
    "import codecs\n",
    "import pprint\n",
    "import psycopg2\n",
    "from psycopg2 import extras as ext\n",
    "import sql_statements\n",
    "from datetime import datetime\n",
    "import time\n",
    "from decimal import *\n",
    "\n",
    "CREATE_TABLE_STMT = sql_statements.CREATE_TABLE_STMT\n",
    "INSERT_TWEET_STMT = sql_statements.INSERT_TWEET_STMT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configure parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text search\n",
    "This will search the full text of the tweet, any retweeted_status text, and any quoted_status text.\n",
    "\n",
    "`search_text`: set to True if you want to use text search\n",
    "\n",
    "`keywords`: add the keywords you want to match here\n",
    "\n",
    "`all_keywords`: whether to check for all keywords. If true, it will match only tweets that have all keywords. If false it will check whether any of the keywords exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = True\n",
    "keywords = [\"white\", \"helmet\"]\n",
    "all_keywords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date bounds\n",
    "This will only match tweets within the given date bounds\n",
    "\n",
    "`match_dates`: whether to use date bounds\n",
    "\n",
    "`bounds`: the date bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dates = True\n",
    "bounds = (datetime(2017, 5, 27, 0, 0, 0), datetime(2018, 3, 2, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex match\n",
    "This will regex match the full text of the tweet, any retweeted_status text, and any quoted_status text\n",
    "\n",
    "`use_regex_match`: whether to use regex matching\n",
    "\n",
    "`reg_expr`: the regex expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_regex_match = False\n",
    "reg_expr = \"Leo doesn't understand regex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "`folders`: Folders where the json files are (it will process all the json files in each folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"/data/captures/Syria composite - Restart 2018/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database configuration\n",
    "The file should look like:\n",
    "```\n",
    "host = INSERT_HOSTNAME\n",
    "username = INSERT_USERNAME\n",
    "password = INSERT_PASSWORD\n",
    "```\n",
    "Make sure that the database exists (you might have to run ```CREATE DATABASE database_name;```)\n",
    "\n",
    "`database_name` is the name of the database\n",
    "\n",
    "`db_config_file` is the path to the file with the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"syria_composite_white_helmets\"\n",
    "db_config_file = \"/home/lgs17/config/bowker_config.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    ## Replace weird characters that make Postgres unhappy\n",
    "    return s.replace(\"\\x00\", \"\") if s else None\n",
    "\n",
    "## Get a value from the given dictionary by following the path\n",
    "## If the path isn't valid, nothing will be returned\n",
    "def get_nested_value(outer_dict, path_str, default=None):\n",
    "    path = path_str.split(\".\") # get a list of nested dictionary keys (the path)\n",
    "    cur_dict = outer_dict\n",
    "\n",
    "    ## step through the path and try to process it\n",
    "    try:\n",
    "        for step in path:\n",
    "            ## If it's actually a list index, convert it to an integer\n",
    "            if step.isdigit():\n",
    "                step = int(step)\n",
    "\n",
    "            ## Get the nested value associated with that key\n",
    "            cur_dict = cur_dict[step]\n",
    "\n",
    "        ## Once it's at the end of the path, return the nested value\n",
    "        return cur_dict\n",
    "\n",
    "    ## The value didn't exist\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        pass\n",
    "\n",
    "    return default\n",
    "\n",
    "\n",
    "## Get a json string rather than an individual value\n",
    "def get_nested_value_json(_dict, path, default=None):\n",
    "    ## Pull the nested value\n",
    "    value = get_nested_value(_dict, path, default)\n",
    "\n",
    "    ## Return a string of the json dictionary\n",
    "    if value:\n",
    "        return json.dumps(value)\n",
    "\n",
    "\n",
    "## Given a string and a list of keywords, returns all keywords that exist in the string (case-insensitive)\n",
    "## To check for any matches, just see if there are things in the list\n",
    "def get_matching_keywords(search_string, keywords):\n",
    "    return [keyword for keyword in keywords if keyword in search_string.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing full text\n",
    "Some tweets have been truncated and have and additional `full_text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pulls the full text field depending on whether the tweet has been truncated\n",
    "def get_full_text(tweet):\n",
    "    if tweet[\"truncated\"] and \"retweeted_status\" in tweet:\n",
    "        ## If it's a retweet, pull the full text from the original tweet because that's more reliable\n",
    "        return get_full_text(tweet[\"retweeted_status\"])\n",
    "\n",
    "    elif tweet[\"truncated\"]:\n",
    "        ## Grab the full text if it exists\n",
    "        return tweet[\"extended_tweet\"][\"full_text\"]\n",
    "\n",
    "    else:\n",
    "        ## Otherwise just use the text\n",
    "        return tweet[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering individual tweets\n",
    "This is where all the matching is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_matches_parameters(tweet):\n",
    "\n",
    "    #######################\n",
    "    ## Keyword filtering ##\n",
    "    #######################\n",
    "    \n",
    "    if search_text:\n",
    "        def matches_keywords(text):\n",
    "            matches = get_matching_keywords(text)\n",
    "\n",
    "            if all_keywords:\n",
    "                return matches == keywords ## only return True if all keywords matched\n",
    "\n",
    "            else:\n",
    "                return bool(matches) ## return True if there's at least one match\n",
    "\n",
    "\n",
    "        ## Make a list of fields to check for keyword matches\n",
    "        keyword_texts = [get_full_text(tweet)]\n",
    "\n",
    "        if \"retweeted_status\" in tweet:\n",
    "            keyword_texts.append(get_full_text(tweet[\"retweeted_status\"]))\n",
    "\n",
    "        if \"quoted_status\" in tweet:\n",
    "            keyword_texts.append(get_full_text(tweet[\"quoted_status\"]))\n",
    "\n",
    "        keywords_matches = [matches_keywords(text, keywords) for text in keyword_texts]\n",
    "        if not any(keyword_matches):\n",
    "            return False\n",
    "\n",
    "    #############################\n",
    "    ## Time interval filtering ##\n",
    "    #############################\n",
    "    \n",
    "    if match_dates:\n",
    "        created_at = get_nested_value(tweet, \"created_at\")\n",
    "        created_ts = datetime.strptime(created_at[0:19]+created_at[25:], \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "        if not created_ts or created_ts < bounds[0] or created_ts > bounds[1]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "    ####################\n",
    "    ## Regex matching ##\n",
    "    ####################\n",
    "    \n",
    "    if use_regex_match:\n",
    "        ## Make a list of fields to check for keyword matches\n",
    "        regex_texts = [get_full_text(tweet)]\n",
    "\n",
    "        if \"retweeted_status\" in tweet:\n",
    "            regex.append(get_full_text(tweet[\"retweeted_status\"]))\n",
    "\n",
    "        if \"quoted_status\" in tweet:\n",
    "            regex_texts.append(get_full_text(tweet[\"quoted_status\"]))\n",
    "        \n",
    "        regex_matches = [bool(re.search(reg_expr, text)) for text in text]\n",
    "        if not any(regex_matches):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting individual tweets\n",
    "This parses the JSON into a row that can be inserted into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet(tweet):\n",
    "    ## Adding everything to a tuple and inserting the tuple to the database\n",
    "    ucts = get_nested_value(tweet, \"user.created_at\")\n",
    "    user_created_ts = datetime.strptime(ucts[0:19]+ucts[25:], \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "    item = (\n",
    "        tweet[\"id\"],\n",
    "        tweet[\"created_at\"],\n",
    "        created_ts,\n",
    "        get_nested_value(tweet, \"lang\"),\n",
    "        clean(get_nested_value(tweet, \"text\")),\n",
    "        clean(get_nested_value(tweet, \"extended_tweet.full_text\")),\n",
    "        get_nested_value(tweet, \"coordinates.coordinates{0}\"),\n",
    "        get_nested_value(tweet, \"coordinates.coordinates{1}\"),\n",
    "        get_nested_value_json(tweet, \"contributors\"),\n",
    "        get_nested_value_json(tweet, \"counts\"),\n",
    "        get_nested_value_json(tweet, \"entities\"),\n",
    "        get_nested_value(tweet, \"entities.urls.0.expanded_url\"),\n",
    "        get_nested_value_json(tweet, \"entities.urls\"),\n",
    "        get_nested_value(tweet, \"filter_level\"),\n",
    "        get_nested_value_json(tweet, \"coordinates\"),\n",
    "        get_nested_value_json(tweet, \"place\"),\n",
    "        get_nested_value(tweet, \"possibly_sensitive\"),\n",
    "        get_nested_value_json(tweet, \"user\"),\n",
    "        get_nested_value(tweet, \"user.id\"),\n",
    "        get_nested_value(tweet, \"user.screen_name\"),\n",
    "        get_nested_value(tweet, \"user.followers_count\"),\n",
    "        get_nested_value(tweet, \"user.friends_count\"),\n",
    "        get_nested_value(tweet, \"user.statuses_count\"),\n",
    "        get_nested_value(tweet, \"user.favourites_count\"),\n",
    "        get_nested_value(tweet, \"user.geo_enabled\"),\n",
    "        get_nested_value(tweet, \"user.time_zone\"),\n",
    "        clean(get_nested_value(tweet, \"user.description\")),\n",
    "        get_nested_value(tweet, \"user.location\"),\n",
    "        get_nested_value(tweet, \"user.created_at\"),\n",
    "        user_created_ts,\n",
    "        get_nested_value(tweet, \"user.lang\"),\n",
    "        get_nested_value(tweet, \"user.listed_count\"),\n",
    "        get_nested_value(tweet, \"user.name\"),\n",
    "        get_nested_value(tweet, \"user.url\"),\n",
    "        get_nested_value(tweet, \"user.utc_offset\"),\n",
    "        get_nested_value(tweet, \"user.verified\"),\n",
    "        get_nested_value(tweet, \"user.contributors_enabled\"),\n",
    "        get_nested_value(tweet, \"user.default_profile\"),\n",
    "        get_nested_value(tweet, \"user.is_translator\"),\n",
    "        get_nested_value(tweet, \"retweet_count\"),\n",
    "        get_nested_value(tweet, \"favorite_count\"),\n",
    "        get_nested_value_json(tweet, \"retweeted_status\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.id\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.screen_name\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.retweet_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.id\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.time_zone\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.friends_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.statuses_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.followers_count\"),\n",
    "        get_nested_value(tweet, \"source\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_screen_name\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_status_id\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_user_id\"),\n",
    "        get_nested_value(tweet, \"quoted_status_id\"),\n",
    "        get_nested_value(tweet, \"quoted_status_id_str\"),\n",
    "        get_nested_value_json(tweet, \"quoted_status\"),\n",
    "        get_nested_value(tweet, \"truncated\"),\n",
    "        get_nested_value(tweet, \"quoted_status.user.screen_name\"),\n",
    "        clean(get_nested_value(tweet, \"retweeted_status.user.description\")),\n",
    "        clean(get_nested_value(tweet, \"quoted_status.user.description\")),\n",
    "        json.dumps(tweet))\n",
    "  \n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the tweets in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_file(json_file_path, cursor, database, keywords):\n",
    "    queue = []\n",
    "\n",
    "    with open(json_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            ## Skip empty lines\n",
    "            if (not line or len(line) < 2):\n",
    "                continue  \n",
    "\n",
    "            tweet = None\n",
    "\n",
    "            ## Load the tweet string into a dictionary.\n",
    "            ## There's like one tweet in one json file that is bad json, so I've just been skipping\n",
    "            ## it. If there end up being a lot, we should probably figure out why that's happening.\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "            except ValueError:\n",
    "                print(\"bad json\")\n",
    "                print(line)\n",
    "                continue\n",
    "\n",
    "            ## Make sure that the tweet matches all filtering parameters\n",
    "            if not tweet_matches_parameters:\n",
    "                continue\n",
    "\n",
    "            tweet_row = extract_tweet(tweet)\n",
    "            if tweet_row:\n",
    "                queue.append(tweet_row)\n",
    "\n",
    "    ## Insert the extracted tweets into the database\n",
    "    ext.execute_batch(cursor, INSERT_TWEET_STMT, queue)\n",
    "\n",
    "    ## Just to keep track of how many have been inserted\n",
    "    return len(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse the database credentials out of the file\n",
    "config = {\"database\": database_name}\n",
    "for line in open(db_config_file).readlines():\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    config[key] = value\n",
    "\n",
    "## Connect to the database and get a cursor object\n",
    "database = psycopg2.connect(**config)\n",
    "cursor = database.cursor()\n",
    "\n",
    "cursor.execute(CREATE_TABLE_STMT)\n",
    "database.commit()\n",
    "\n",
    "## Uncomment to clear the table each time the script starts\n",
    "cursor.execute(\"DELETE FROM tweets\")\n",
    "\n",
    "## Keep track of how many tweets have been inserted (just make sure it's running)\n",
    "total = 0\n",
    "\n",
    "## Process each folder\n",
    "for folder_path in folders:\n",
    "    ## Make sure only valid .json files are processed\n",
    "    json_files_to_process = [json_file for json_file in os.listdir(folder_path) if json_file[-5:] == \".json\"]\n",
    "\n",
    "    for j in range(len(json_files_to_process)):\n",
    "        json_file = json_files_to_process[j]\n",
    "        ## For each file, extract the tweets and add the number extracted to the total\n",
    "        total += extract_json_file(os.path.join(folder_path, json_file), cursor, database, keywords)\n",
    "        print(\"{fnum}/{ftotal}: {tnum} total tweets inserted\".format(fnum=j, ftotal=(len(json_files_to_process)+1), tnum=total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "## Close everything\n",
    "cursor.close()\n",
    "database.commit()\n",
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
