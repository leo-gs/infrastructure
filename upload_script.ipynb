{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import extras as ext\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import urllib2\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "CREATE_TABLE_STMT = sql_statements.CREATE_TABLE_STMT\n",
    "INSERT_TWEET_STMT = sql_statements.INSERT_TWEET_STMT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configure parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text search\n",
    "This will search the full text of the tweet, any retweeted_status text, and any quoted_status text.\n",
    "\n",
    "`search_text`: set to True if you want to use text search\n",
    "\n",
    "`keywords`: add the keywords you want to match here\n",
    "\n",
    "`all_keywords`: whether to check for all keywords. If true, it will match only tweets that have all keywords. If false it will check whether any of the keywords exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = True\n",
    "keywords = [\"nato\"]\n",
    "all_keywords = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date bounds\n",
    "This will only match tweets within the given date bounds\n",
    "\n",
    "`match_dates`: whether to use date bounds\n",
    "\n",
    "`bounds`: the date bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dates = False\n",
    "bounds = (datetime(2017, 5, 27, 0, 0, 0), datetime(2018, 3, 2, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex match\n",
    "This will regex match the full text of the tweet, any retweeted_status text, and any quoted_status text\n",
    "\n",
    "`use_regex_match`: whether to use regex matching\n",
    "\n",
    "`reg_expr`: the regex expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_regex_match = False\n",
    "reg_expr = \"Leo doesn't understand regex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "`folders`: Folders where the json files are (it will process all the json files in each folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"/var/collect/twcap/captures/Disinfo 2/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database configuration\n",
    "The file should look like:\n",
    "```\n",
    "host = INSERT_HOSTNAME\n",
    "username = INSERT_USERNAME\n",
    "password = INSERT_PASSWORD\n",
    "```\n",
    "Make sure that the database exists (you might have to run ```CREATE DATABASE database_name;```)\n",
    "\n",
    "`database_name` is the name of the database\n",
    "\n",
    "`db_config_file` is the path to the file with the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"disinfo_2_nato\"\n",
    "db_config_file = \"/home/lgs17/bowker_config.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    ## Replace weird characters that make Postgres unhappy\n",
    "    return s.replace(\"\\x00\", \"\") if s else None\n",
    "\n",
    "## Get a value from the given dictionary by following the path\n",
    "## If the path isn't valid, nothing will be returned\n",
    "def get_nested_value(outer_dict, path_str, default=None):\n",
    "    path = path_str.split(\".\") # get a list of nested dictionary keys (the path)\n",
    "    cur_dict = outer_dict\n",
    "\n",
    "    ## step through the path and try to process it\n",
    "    try:\n",
    "        for step in path:\n",
    "            ## If it's actually a list index, convert it to an integer\n",
    "            if step.isdigit():\n",
    "                step = int(step)\n",
    "\n",
    "            ## Get the nested value associated with that key\n",
    "            cur_dict = cur_dict[step]\n",
    "\n",
    "        ## Once it's at the end of the path, return the nested value\n",
    "        return cur_dict\n",
    "\n",
    "    ## The value didn't exist\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        pass\n",
    "\n",
    "    return default\n",
    "\n",
    "\n",
    "## Get a json string rather than an individual value\n",
    "def get_nested_value_json(_dict, path, default=None):\n",
    "    ## Pull the nested value\n",
    "    value = get_nested_value(_dict, path, default)\n",
    "\n",
    "    ## Return a string of the json dictionary\n",
    "    if value:\n",
    "        return json.dumps(value)\n",
    "\n",
    "\n",
    "## Given a string and a list of keywords, returns all keywords that exist in the string (case-insensitive)\n",
    "## To check for any matches, just see if there are things in the list\n",
    "def get_matching_keywords(search_string):\n",
    "    return [keyword for keyword in keywords if keyword in search_string.lower()]\n",
    "\n",
    "\n",
    "known_urls = {}\n",
    "## Expanding a URL\n",
    "def expand_url(tweet, index=0, try_threshold=1):\n",
    "    \n",
    "    ## Two different ways to try to expand urls\n",
    "    def expand_url_1(url):\n",
    "        res = None\n",
    "        try:\n",
    "            return urllib2.urlopen(url).url\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def expand_url_2(url):\n",
    "        try:\n",
    "            return requests.get(url).url\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    ## If the tweet is truncated, we want to use the urls in the extended_tweet field\n",
    "    if tweet[\"truncated\"]:\n",
    "        tweet = tweet[\"extended_tweet\"]\n",
    "    \n",
    "    ## Pull out the url at the given index (if it exists - otherwise return None)\n",
    "    url_json = tweet[\"entities\"].get(\"urls\")\n",
    "    if not url_json or index >= len(url_json):\n",
    "        return None\n",
    "    url = url_json[index][\"expanded_url\"]\n",
    "    \n",
    "    ## If it's a Twitter url, so we dont need to expand it any farther\n",
    "    if \"https://twitter.com/\" in url:\n",
    "        return url\n",
    "    \n",
    "    ## If we've already looked it up, use that result\n",
    "    if url in known_urls:\n",
    "        return known_urls[url]\n",
    "    \n",
    "    ## Otherwise, try the first method and return it if it works\n",
    "    expanded_url = expand_url_1(url)\n",
    "    if expanded_url:\n",
    "        return expanded_url\n",
    "\n",
    "    ## The first method didn't work, so try the second method\n",
    "    expanded_url = expand_url_2(url)\n",
    "    if expanded_url:\n",
    "        return expanded_url\n",
    "    \n",
    "    ## We weren't successful in expanding it, so just return the original\n",
    "    expanded_url = url\n",
    "    known_urls[url] = expanded_url\n",
    "    return expanded_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing full text\n",
    "Some tweets have been truncated and have and additional `full_text` field. Additionally, we want to reconstruct quoted tweets and retweets so they appear like they would in a user's feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_text(tweet):\n",
    "\n",
    "    ## Encode unicode so it plays nice with the string formatting\n",
    "    def c(u):\n",
    "        return u.encode('utf8')\n",
    "\n",
    "    tweet_complete_text = tweet[\"text\"]\n",
    "    if tweet[\"truncated\"]:\n",
    "        ## Applicable to original tweets and commentary on quoted tweets\n",
    "        tweet_complete_text = tweet[\"extended_tweet\"][\"full_text\"]\n",
    "\n",
    "    # this handles retweets of original tweets and retweets of quoted tweets\n",
    "    if \"retweeted_status\" in tweet:\n",
    "        return \"RT @{username}: {orig_complete_text}\".format(\n",
    "            username=c(tweet[\"retweeted_status\"][\"user\"][\"screen_name\"]),\n",
    "            orig_complete_text=get_complete_text(tweet[\"retweeted_status\"]))\n",
    "\n",
    "    # I am fairly certain that the only way you can quote a tweet is by quoting the original tweet; i.e. I don't think you can quote a retweet\n",
    "    elif \"quoted_status\" in tweet:\n",
    "        return \"{qt_complete_text} QT @{username}: {orig_complete_text}\".format(\n",
    "            qt_complete_text=c(tweet_complete_text),\n",
    "            username=c(tweet[\"quoted_status\"][\"user\"][\"screen_name\"]),\n",
    "            orig_complete_text=get_complete_text(tweet[\"quoted_status\"]))\n",
    "    else:\n",
    "        return c(tweet_complete_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering individual tweets\n",
    "This is where all the matching is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_parameters(tweet):\n",
    "    \n",
    "    #######################\n",
    "    ## Keyword filtering ##\n",
    "    #######################\n",
    "    \n",
    "    if search_text:\n",
    "        def matches_keywords(text):\n",
    "            matches = get_matching_keywords(text)\n",
    "\n",
    "            if all_keywords:\n",
    "                return matches == keywords ## only return True if all keywords matched\n",
    "\n",
    "            else:\n",
    "                return bool(matches) ## return True if there's at least one match\n",
    "\n",
    "\n",
    "        ## Make a list of fields to check for keyword matches (could add user_description, etc.)\n",
    "        keyword_texts = [get_complete_text(tweet)]\n",
    "\n",
    "        keyword_matches = [matches_keywords(keyword_text) for keyword_text in keyword_texts]\n",
    "        if not any(keyword_matches):\n",
    "            return False\n",
    "\n",
    "    #############################\n",
    "    ## Time interval filtering ##\n",
    "    #############################\n",
    "    \n",
    "    if match_dates:\n",
    "        created_at = get_nested_value(tweet, \"created_at\")\n",
    "        created_ts = datetime.strptime(created_at[0:19]+created_at[25:], \"%a %b %d %H:%M:%S %Y\")\n",
    "        \n",
    "        if not created_ts or created_ts < bounds[0] or created_ts > bounds[1]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "    ####################\n",
    "    ## Regex matching ##\n",
    "    ####################\n",
    "    \n",
    "    if use_regex_match:\n",
    "        ## Make a list of fields to check for keyword matches\n",
    "        regex_texts = [get_complete_text(tweet)]\n",
    "        \n",
    "        regex_matches = [bool(re.search(reg_expr, text)) for text in text]\n",
    "        if not any(regex_matches):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting individual tweets\n",
    "This parses the JSON into a row that can be inserted into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet(tweet):\n",
    "    ## Adding everything to a huge tuple and inserting the tuple to the database\n",
    "    created_at = get_nested_value(tweet, \"created_at\")\n",
    "    created_ts = datetime.strptime(created_at[0:19]+created_at[25:], \"%a %b %d %H:%M:%S %Y\")\n",
    "    \n",
    "    ucts = get_nested_value(tweet, \"user.created_at\")\n",
    "    user_created_ts = datetime.strptime(ucts[0:19]+ucts[25:], \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "    item = (\n",
    "        tweet[\"id\"],\n",
    "        created_at,\n",
    "        created_ts,\n",
    "        get_nested_value(tweet, \"lang\"),\n",
    "        clean(get_nested_value(tweet, \"text\")),\n",
    "        clean(get_complete_text(tweet)),\n",
    "        get_nested_value(tweet, \"coordinates.coordinates{0}\"),\n",
    "        get_nested_value(tweet, \"coordinates.coordinates{1}\"),\n",
    "        get_nested_value_json(tweet, \"contributors\"),\n",
    "        get_nested_value_json(tweet, \"counts\"),\n",
    "        get_nested_value_json(tweet, \"entities\"),\n",
    "        expand_url(tweet, index=0),\n",
    "        get_nested_value_json(tweet, \"entities.urls\"),\n",
    "        get_nested_value(tweet, \"filter_level\"),\n",
    "        get_nested_value_json(tweet, \"coordinates\"),\n",
    "        get_nested_value_json(tweet, \"place\"),\n",
    "        get_nested_value(tweet, \"possibly_sensitive\"),\n",
    "        get_nested_value_json(tweet, \"user\"),\n",
    "        get_nested_value(tweet, \"user.id\"),\n",
    "        get_nested_value(tweet, \"user.screen_name\"),\n",
    "        get_nested_value(tweet, \"user.followers_count\"),\n",
    "        get_nested_value(tweet, \"user.friends_count\"),\n",
    "        get_nested_value(tweet, \"user.statuses_count\"),\n",
    "        get_nested_value(tweet, \"user.favourites_count\"),\n",
    "        get_nested_value(tweet, \"user.geo_enabled\"),\n",
    "        get_nested_value(tweet, \"user.time_zone\"),\n",
    "        clean(get_nested_value(tweet, \"user.description\")),\n",
    "        get_nested_value(tweet, \"user.location\"),\n",
    "        get_nested_value(tweet, \"user.created_at\"),\n",
    "        user_created_ts,\n",
    "        get_nested_value(tweet, \"user.lang\"),\n",
    "        get_nested_value(tweet, \"user.listed_count\"),\n",
    "        get_nested_value(tweet, \"user.name\"),\n",
    "        get_nested_value(tweet, \"user.url\"),\n",
    "        get_nested_value(tweet, \"user.utc_offset\"),\n",
    "        get_nested_value(tweet, \"user.verified\"),\n",
    "        get_nested_value(tweet, \"user.contributors_enabled\"),\n",
    "        get_nested_value(tweet, \"user.default_profile\"),\n",
    "        get_nested_value(tweet, \"user.is_translator\"),\n",
    "        get_nested_value(tweet, \"retweet_count\"),\n",
    "        get_nested_value(tweet, \"favorite_count\"),\n",
    "        get_nested_value_json(tweet, \"retweeted_status\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.id\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.screen_name\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.retweet_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.id\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.time_zone\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.friends_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.statuses_count\"),\n",
    "        get_nested_value(tweet, \"retweeted_status.user.followers_count\"),\n",
    "        get_nested_value(tweet, \"source\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_screen_name\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_status_id\"),\n",
    "        get_nested_value(tweet, \"in_reply_to_user_id\"),\n",
    "        get_nested_value(tweet, \"quoted_status_id\"),\n",
    "        get_nested_value(tweet, \"quoted_status_id_str\"),\n",
    "        get_nested_value_json(tweet, \"quoted_status\"),\n",
    "        get_nested_value(tweet, \"truncated\"),\n",
    "        get_nested_value(tweet, \"quoted_status.user.screen_name\"),\n",
    "        clean(get_nested_value(tweet, \"retweeted_status.user.description\")),\n",
    "        clean(get_nested_value(tweet, \"quoted_status.user.description\")),\n",
    "        json.dumps(tweet))\n",
    "  \n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the tweets in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_file(json_file_path, cursor, database, keywords):\n",
    "    with open(json_file_path, 'r') as infile:\n",
    "        queue = []\n",
    "        lines = [line for line in infile if (line and len(line) < 2)]\n",
    "\n",
    "        for line in lines:\n",
    "            tweet = None\n",
    "\n",
    "            ## Load the tweet string into a dictionary.\n",
    "            ## There's like one tweet in one json file that is bad json, so I've just been skipping\n",
    "            ## it. If there end up being a lot, we should probably figure out why that's happening.\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "                \n",
    "                ## Make sure that the tweet matches all filtering parameters\n",
    "                if matches_parameters(tweet):\n",
    "                    tweet_row = extract_tweet(tweet)\n",
    "                    \n",
    "                    if tweet_row:\n",
    "                        queue.append(tweet_row)\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"bad json\")\n",
    "                print(line)\n",
    "                return\n",
    "        \n",
    "        ## Insert all the extracted tweets into the database\n",
    "        ext.execute_batch(cursor, INSERT_TWEET_STMT, queue)\n",
    "\n",
    "        ## Just to keep track of how many have been inserted\n",
    "        return len(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse the database credentials out of the file\n",
    "config = {\"database\": database_name}\n",
    "for line in open(db_config_file).readlines():\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    config[key] = value\n",
    "\n",
    "## Connect to the database and get a cursor object\n",
    "database = psycopg2.connect(**config)\n",
    "cursor = database.cursor()\n",
    "\n",
    "cursor.execute(CREATE_TABLE_STMT)\n",
    "database.commit()\n",
    "\n",
    "## Uncomment to clear the table each time the script starts\n",
    "cursor.execute(\"DELETE FROM tweets\")\n",
    "\n",
    "## Keep track of how many tweets have been inserted (just make sure it's running)\n",
    "total = 0\n",
    "\n",
    "## Process each folder\n",
    "for folder_path in folders:\n",
    "    ## Make sure only valid .json files are processed\n",
    "    json_files_to_process = [json_file for json_file in os.listdir(folder_path) if json_file[-5:] == \".json\"]\n",
    "\n",
    "    for j in range(len(json_files_to_process)):\n",
    "        json_file = json_files_to_process[j]\n",
    "        ## For each file, extract the tweets and add the number extracted to the total\n",
    "        total += extract_json_file(os.path.join(folder_path, json_file), cursor, database, keywords)\n",
    "        print(\"{fnum}/{ftotal}: {tnum} total tweets inserted\".format(fnum=j, ftotal=(len(json_files_to_process)+1), tnum=total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "## Close everything\n",
    "cursor.close()\n",
    "database.commit()\n",
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
