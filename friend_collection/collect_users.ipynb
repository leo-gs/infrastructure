{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters\n",
    "`JSONFile` keeps track of both the filename and its date/time of creation (since there might be multiple files per hour).\n",
    "`start_interval` and `end_interval` are exclusive time bounds on running which JSON files to process.\n",
    "\n",
    "This section first gets a list of all files in the `data_dir` and filters to only those with JSON file extensions.\n",
    "Then it converts all of those to `JSONFile` objects.\n",
    "To filter by time window, it checks whether the `progress_file` exists, and if so, finds the name of the last JSON file processed and uses that to set `start_interval` (otherwise there's no `start_interval`). `end_interval` is set to be the current hour rounded down. I made this decision because the logic is more simple if I process each hour's files at once, so it's easier to wait until the hour is over before processing its files. **TODO: check in the capture code to see when the JSON files are actually dumped.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONFile:\n",
    "\n",
    "    def convert_json_filename_to_datetime(filename, collection_name):\n",
    "        filedate = filename.replace(collection_name, \"\").replace(\".json\", \"\")[:13]\n",
    "        filedate = datetime.strptime(filedate, \"%Y%m%d_%H%M\")\n",
    "        return filedate\n",
    "\n",
    "    def within_interval(self, start_interval, end_interval):\n",
    "        if start_interval:\n",
    "            return (self.filedate > start_interval) and (self.filedate < end_interval)\n",
    "        else:\n",
    "            return self.filedate < end_interval\n",
    "    \n",
    "    def __init__(self, filename, collection_name):\n",
    "        self.filename = filename\n",
    "        self.filedate = JSONFile.convert_json_filename_to_datetime(filename, collection_name)\n",
    "\n",
    "def get_time_window(progress_file, collection_name):\n",
    "    start_interval = None\n",
    "    if os.path.isfile(progress_file):\n",
    "        progress = json.load(open(progress_file))[\"last_file_processed\"]\n",
    "        start_interval = JSONFile.convert_json_filename_to_datetime(progress, collection_name)\n",
    "    \n",
    "    end_interval = datetime.now().replace(minute=0, second=0, microsecond=0) ## Set the interval end to be at the hour\n",
    "    \n",
    "    return start_interval, end_interval\n",
    "\n",
    "def update_progress_file(collection_files, progress_file):\n",
    "    if collection_files:\n",
    "        print(\"dumping\")\n",
    "        last_file = max(collection_files, key=lambda jfile: jfile.filedate)\n",
    "        progress = {\"last_file_processed\": last_file.filename}\n",
    "        json.dump(progress, open(progress_file, \"w+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull user_ids from a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id_list_from_file(filepath):\n",
    "\tuser_ids = []\n",
    "\twith open(filepath) as infile:\n",
    "\t\tfor line in infile:\n",
    "\t\t\tif (not line or len(line) < 2):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttweet = json.loads(line)\n",
    "\t\t\t\tuser_id = tweet[\"user\"][\"id\"]\n",
    "\t\t\t\tuser_ids.append(user_id)\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\tprint(\"bad json: \" + line)\n",
    "\t\t\t\tcontinue\n",
    "\treturn user_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(configuration):\n",
    "    collection_name = configuration[\"collection_name\"]\n",
    "    \n",
    "    ## Get all the JSON files\n",
    "    collection_files = os.listdir(configuration[\"data_dir\"])\n",
    "    collection_files = list(filter(lambda f: f[-5:] == \".json\", collection_files)) ## Filter out .tmp files\n",
    "    collection_files = [JSONFile(f, collection_name) for f in collection_files]\n",
    "    \n",
    "    ## Filter out the already-processed JSON files by time window\n",
    "    progress_file = configuration[\"progress_file\"]\n",
    "    start_interval, end_interval = get_time_window(progress_file, collection_name)\n",
    "    collection_files = list(filter(lambda jfile: jfile.within_interval(start_interval, end_interval), collection_files)) ## Filter to time window\n",
    "    \n",
    "    ## Update the file that stores the time window\n",
    "    update_progress_file(collection_files, progress_file)\n",
    "    \n",
    "    ## Get all the user_ids from the new JSON files\n",
    "    collections_dir = configuration[\"data_dir\"]\n",
    "    user_ids = []\n",
    "    for jfile in collection_files:\n",
    "        user_ids.extend(get_user_id_list_from_file(os.path.join(collections_dir, jfile.filename)))\n",
    "\n",
    "    ## Get user_ids already written to the file (if the file exists)\n",
    "    user_ids_file = configuration[\"user_ids_csv\"]\n",
    "    if os.path.isfile(user_ids_file):\n",
    "        current_user_ids = pd.read_csv(user_ids_file, header=None, names=[\"user_id\"])[\"user_id\"].tolist()\n",
    "        user_ids = user_ids + current_user_ids\n",
    "    \n",
    "    ## Drop any duplicates\n",
    "    user_ids = set(user_ids)\n",
    "\n",
    "    ## Rewrite user_id file\n",
    "    with open(user_ids_file, \"w+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for user_id in user_ids:\n",
    "            writer.writerow([user_id])\n",
    "    \n",
    "    return (user_ids, start_interval, end_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
